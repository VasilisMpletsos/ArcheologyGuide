{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List\n",
    "from transformers import Pipeline, PreTrainedTokenizer\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"### Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "INTRO_BLURB = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    ")\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_WITH_CONTEXT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{input_key}\n",
    "{context}\n",
    "\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    input_key=INPUT_KEY,\n",
    "    context=\"{context}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n",
    "    \"\"\"Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
    "    When training, we configure the tokenizer so that the sequences like \"### Instruction:\" and \"### End\" are\n",
    "    treated specially and converted to a single, new token.  This retrieves the token ID each of these keys map to.\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
    "        key (str): the key to convert to a single token\n",
    "    Raises:\n",
    "        RuntimeError: if more than one ID was generated\n",
    "    Returns:\n",
    "        int: the token ID for the given key\n",
    "    \"\"\"\n",
    "    token_ids = tokenizer.encode(key)\n",
    "    if len(token_ids) > 1:\n",
    "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
    "    return token_ids[0]\n",
    "\n",
    "def preprocess(tokenizer, instruction_text, context_text=None):\n",
    "    if context_text:\n",
    "        prompt_text = PROMPT_WITH_CONTEXT_FOR_GENERATION_FORMAT.format(instruction=instruction_text, context=context_text)\n",
    "    else:\n",
    "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n",
    "    print(prompt_text)\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\",)\n",
    "    inputs[\"prompt_text\"] = prompt_text\n",
    "    inputs[\"instruction_text\"] = instruction_text\n",
    "    inputs[\"context_text\"] = context_text\n",
    "    return inputs\n",
    "\n",
    "def forward(model, tokenizer, model_inputs, max_length=256):\n",
    "    input_ids = model_inputs[\"input_ids\"]\n",
    "    attention_mask = model_inputs.get(\"attention_mask\", None)\n",
    "\n",
    "    if input_ids.shape[1] == 0:\n",
    "        input_ids = None\n",
    "        attention_mask = None\n",
    "        in_b = 1\n",
    "    else:\n",
    "        in_b = input_ids.shape[0]\n",
    "\n",
    "    generated_sequence = model.generate(\n",
    "        input_ids=input_ids.to(model.device),\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    out_b = generated_sequence.shape[0]\n",
    "    generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])\n",
    "    instruction_text = model_inputs.pop(\"instruction_text\", None)\n",
    "\n",
    "    return {\n",
    "        \"generated_sequence\": generated_sequence, \n",
    "        \"input_ids\": input_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "def postprocess(tokenizer, model_outputs):\n",
    "    response_key_token_id = get_special_token_id(tokenizer, RESPONSE_KEY)\n",
    "    end_key_token_id = get_special_token_id(tokenizer, END_KEY)\n",
    "    generated_sequence = model_outputs[\"generated_sequence\"][0]\n",
    "    \n",
    "    # send it to cpu\n",
    "    generated_sequence = generated_sequence.cpu()\n",
    "    generated_sequence = generated_sequence.numpy().tolist()\n",
    "    records = []\n",
    "\n",
    "    for sequence in generated_sequence:\n",
    "        decoded = None\n",
    "\n",
    "        try:\n",
    "            response_pos = sequence.index(response_key_token_id)\n",
    "        except ValueError:\n",
    "            print(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n",
    "            response_pos = None\n",
    "\n",
    "        if response_pos:\n",
    "            try:\n",
    "                end_pos = sequence.index(end_key_token_id)\n",
    "            except ValueError:\n",
    "                print(\"Could not find end key, the output is truncated!\")\n",
    "                end_pos = None\n",
    "            decoded = tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n",
    "            \n",
    "        if not decoded:\n",
    "            # Otherwise we'll decode everything and use a regex to find the response and end.\n",
    "\n",
    "            fully_decoded = tokenizer.decode(sequence)\n",
    "            # The response appears after \"### Response:\".  The model has been trained to append \"### End\" at the\n",
    "            # end.\n",
    "            m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL)\n",
    "            if m:\n",
    "                decoded = m.group(1).strip()\n",
    "            else:\n",
    "                # The model might not generate the \"### End\" sequence before reaching the max tokens.  In this case,\n",
    "                # return everything after \"### Response:\".\n",
    "                m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n",
    "                if m:\n",
    "                    decoded = m.group(1).strip()\n",
    "                else:\n",
    "                    print(f\"Failed to find response in:\\n{fully_decoded}\")\n",
    "            \n",
    "            \n",
    "        rec = {\"generated_text\": decoded}\n",
    "        records.append(rec)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"left\")\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        \"databricks/dolly-v2-3b\", \n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config,\n",
    "        trust_remote_code = True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\", device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Who was the president of United states of America?\n",
      "\n",
      "### Input:\n",
      "The president of United states of America was Barack Obama.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Barack Obama\n"
     ]
    }
   ],
   "source": [
    "text = \"Who was the president of United states of America?\"\n",
    "context = \"The president of United states of America was Barack Obama.\"\n",
    "pre_process_result = preprocess(tokenizer, text, context)\n",
    "model_result = forward(model_4bit, tokenizer, pre_process_result)\n",
    "final_output = postprocess(tokenizer, model_result)\n",
    "print(final_output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

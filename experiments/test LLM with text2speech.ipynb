{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into csv rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove with regex the brackets and its content from a phrase\n",
    "def clean_text(text):\n",
    "    # strip sentenece\n",
    "    text = text.lower().strip()\n",
    "    # remove tabs\n",
    "    text = text.replace('\\t', '')\n",
    "    # remove new lines\n",
    "    text = text.replace('\\n', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = read_txt_file('./KnowledgeBase/general_description.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(    \n",
    "    separator = \".\",\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 50,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 224, which is longer than the specified 150\n",
      "Created a chunk of size 226, which is longer than the specified 150\n"
     ]
    }
   ],
   "source": [
    "passages = text_splitter.create_documents([item_data]);\n",
    "passages = [clean_text(sentence.page_content) for sentence in passages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it is considered as the ancient capital of the island. it was inhabited from the 12th century bc until the early middle ages',\n",
       " 'it was called kythnos in the ancient times and the island was named after it.it is also referred to as ovriokastro and rigokastro',\n",
       " 'vryokastro is on the northwestern side of the island, between the bays of merichas and apokrisi',\n",
       " 'the town had an area of 300 acres and was surrounded by city walls',\n",
       " 'including the islet \"vryokastraki\", which was formerly connected to the coast by a narrow isthmus',\n",
       " 'underwater surveys and excavations show that before the sea level rise, which resulted in the separation of the rocky islet from the coast, there were buildings which are now in the water along with the now underwater walls',\n",
       " 'it is the most fully researched and excavated site of the island.the excavation of the area is ongoing',\n",
       " \"to date, public buildings, an aqueduct, an acropolis, two necropolises, a harbor, a sanctuary, a temple, a monumental mound and a large building of the classical and hellenistic period have been found in the settlement's area\",\n",
       " 'vryokastraki was excavated from 2018 to 2020 and its findings document the human presence in the 12th century bc on the island',\n",
       " 'in the upper city a sanctuary complex was found that probably were temples for asclepius, cabeiri, aphrodite, apollo, and artemis',\n",
       " \"west of the sanctuary the city's agora during the hellenistic period was found.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "df = pd.DataFrame({'passages': passages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./KnowledgeBase/general_passages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     Loaded Similarity Model\n"
     ]
    }
   ],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "similarity_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "similarity_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print('INFO:     Loaded Similarity Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "building1_passages = pd.read_csv('../data/building1_passages.csv')['passages'].to_list()\n",
    "building2_passages = pd.read_csv('../data/building2_passages.csv')['passages'].to_list()\n",
    "building3_passages = pd.read_csv('../data/building3_passages.csv')['passages'].to_list()\n",
    "building5_passages = pd.read_csv('../data/building5_passages.csv')['passages'].to_list()\n",
    "\n",
    "building_passages = [building1_passages, building2_passages, building3_passages, building5_passages]\n",
    "building_embeddings = []\n",
    "for i, passages in enumerate(building_passages):\n",
    "    encoded_input = similarity_tokenizer(building_passages[i], padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = similarity_model(**encoded_input)\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    building_embeddings.append(embeddings.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11622969,  0.03409467,  0.05019226, ...,  0.05629298,\n",
       "         0.11818867,  0.02524624],\n",
       "       [-0.11978338,  0.40054923, -0.1928807 , ...,  0.07970957,\n",
       "         0.32337907,  0.02805086],\n",
       "       [ 0.38470966,  0.06254954,  0.06127506, ...,  0.5766263 ,\n",
       "        -0.35689756, -0.12071333],\n",
       "       ...,\n",
       "       [-0.021469  ,  0.2774418 ,  0.11278592, ..., -0.01957736,\n",
       "        -0.28363362,  0.17731604],\n",
       "       [ 0.27422208,  0.22551264,  0.19516474, ...,  0.02150778,\n",
       "        -0.09390647, -0.00687842],\n",
       "       [-0.23764972,  0.19658478, -0.49915496, ...,  0.04694569,\n",
       "         0.15331982, -0.00156602]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "building_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buildings one and two seem to be dated to the late classical to early hellenistic times, a period where there seems to be a plan to commemorate the upper city, with the construction of monumental buildings'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"To when it is dated?\"\n",
    "tokenized_query = similarity_tokenizer(question, padding=True, truncation=True, return_tensors='pt')\n",
    "embedded_query = similarity_model(**tokenized_query)\n",
    "question_embedding = mean_pooling(embedded_query, tokenized_query['attention_mask'])\n",
    "question_embedding = question_embedding.detach().numpy()\n",
    "similarities = cosine_similarity(question_embedding, building_embeddings[1])\n",
    "most_similar_passage_index = np.argmax(similarities)\n",
    "building_passages[1][most_similar_passage_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14941174"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awesome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
